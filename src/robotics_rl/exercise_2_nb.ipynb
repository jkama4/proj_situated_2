{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef90dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import SAC, HER\n",
    "\n",
    "import gymnasium_robotics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c720ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"FetchPushDense-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b27f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b2a9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    from schemas import make_env\n",
    "    env = make_env(env_id)\n",
    "else:\n",
    "    from stable_baselines3.common.monitor import Monitor\n",
    "    gym.register_envs(gymnasium_robotics)\n",
    "    env = Monitor(gym.make(env_id, render_mode=\"human\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0d5ee",
   "metadata": {},
   "source": [
    "# Training SAC with optimised hyperparams from Exercise 1\n",
    "First, we'll train a baseline SAC with optimised hyperparms from exercise 1 (except for noise_type and noise_std, since they are not considered by SAC, only by DDPG and TD3). We'll use the recommend 1000000 traing steps. \n",
    "\n",
    "In addition to our earlier parameter, we will use gamma (a discount factor) and tau (soft update coefficient). We used gamma=0.95 because FetchPush has short episodes (50 steps). A lower discount factor encourages the agent to complete the task quickly rather than taking unnecessary steps. We used Ï„=0.005 for soft target updates, which provides stable Q-value targets while still allowing the target network to track improvements in the main network. This is a standard value for continuous control tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f1448a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "def create_and_train_sac_model(env):\n",
    "    model = SAC(\n",
    "        policy=\"MultiInputPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=0.0010108124085550568,\n",
    "        batch_size=256,\n",
    "        buffer_size=1_000_000,\n",
    "        tau=0.005,          \n",
    "        gamma=0.95,\n",
    "        train_freq=1,\n",
    "        gradient_steps=1,\n",
    "        learning_starts=1000,\n",
    "        policy_kwargs=dict(net_arch=[400, 300]),\n",
    "        device=device,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    eval_callback = EvalCallback(\n",
    "        env,\n",
    "        eval_freq=10_000,\n",
    "        n_eval_episodes=10,\n",
    "        log_path=\"logs/reward_shaping\",\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=1_000_000, callback=eval_callback)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9be8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_and_train_sac_model(env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc3311",
   "metadata": {},
   "source": [
    "# Designing Wrappers to Facilitate Learning\n",
    "Since SAC by itself is not enough, we need to facilitate learning by adding some constraints or rules that assist the agent during training. For this purpose, we will use reward shaping.\n",
    "\n",
    "- **Reward Shaping:** adding extra rewards for...\n",
    "    - ... gripper getting close to the object.\n",
    "    - ... object moving toward the goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99066cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShapingWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, w_gripper=0.5, w_goal=1.0):\n",
    "        super().__init__(env)\n",
    "        self.w_gripper = w_gripper\n",
    "        self.w_goal = w_goal\n",
    "        self.prev_potential = None\n",
    "        \n",
    "        self.episode_shaped_reward = 0\n",
    "        self.episode_count = 0\n",
    "    \n",
    "    def compute_potential(self, observation):\n",
    "        gripper_pos = observation[\"observation\"][0:3]\n",
    "        block_pos = observation[\"achieved_goal\"]\n",
    "        goal_pos = observation[\"desired_goal\"]\n",
    "        \n",
    "        d_grip_block = np.linalg.norm(gripper_pos - block_pos)\n",
    "        d_block_goal = np.linalg.norm(block_pos - goal_pos)\n",
    "        \n",
    "        return -(self.w_gripper * d_grip_block + self.w_goal * d_block_goal)\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        if self.prev_potential is not None:\n",
    "            self.episode_count += 1\n",
    "            if self.episode_count % 100 == 0:\n",
    "                print(f\"Episode {self.episode_count}, Shaped reward: {self.episode_shaped_reward:.2f}\")\n",
    "        \n",
    "        self.episode_shaped_reward = 0\n",
    "        \n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        self.prev_potential = self.compute_potential(observation)\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        potential = self.compute_potential(observation)\n",
    "        shaping_reward = potential - self.prev_potential\n",
    "        self.prev_potential = potential\n",
    "        \n",
    "        self.episode_shaped_reward += shaping_reward\n",
    "        \n",
    "        reward += shaping_reward\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0687a15f",
   "metadata": {},
   "source": [
    "Now, we update the environment using the wrapper, and check whether it will increase in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb7d412e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RewardShapingWrapper<Float32Wrapper<Monitor<TimeLimit<OrderEnforcing<PassiveEnvChecker<MujocoFetchPushEnv<FetchPushDense-v4>>>>>>>>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_env = RewardShapingWrapper(env=env)\n",
    "updated_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "939bc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_and_train_sac_model(env=updated_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics-rl-py3.13 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
