{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Situated AI Assignment 2: Robotics and Reinforcement Learning\n",
    "\n",
    "This notebook provides a quickstart for training RL agents on Gymnasium Robotics environments using RL Baselines3 Zoo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup\n",
    "\n",
    "Run these cells once to install dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install system dependencies (Colab only - skip if running locally)\n",
    "# !apt-get update && apt-get install -q -y swig cmake ffmpeg freeglut3-dev xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup virtual display for video recording (Colab only)\n",
    "# import os\n",
    "# os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "# os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mount your drive to the session\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Python packages\n",
    "# !pip install -q rl-zoo3\n",
    "# !pip install -q -e git+https://github.com/Farama-Foundation/Gymnasium-Robotics.git#egg=gymnasium-robotics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create wrapper for record_video (rl_zoo3.record_video doesn't support --gym-packages)\n",
    "# with open('record_video.py', 'w') as f:\n",
    "#     f.write(\n",
    "#         '#!/usr/bin/env python\\n'\n",
    "#         'import gymnasium_robotics\\n'\n",
    "#         'import runpy\\n'\n",
    "#         'runpy.run_module(\"rl_zoo3.record_video\", run_name=\"__main__\")\\n'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Configure Hyperparameters\n",
    "\n",
    "RL Zoo expects hyperparameters in a YAML file. Modify these to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "\n",
    "from optuna import trial, study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_timesteps(trial: trial.Trial) -> int:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-20 15:39:00,518]\u001b[0m A new study created in memory with name: no-name-7e1cf8d2-ed72-45d7-859d-2927720c438c\u001b[0m\n",
      "\u001b[32m[I 2026-01-20 15:39:00,520]\u001b[0m Trial 0 finished with value: 1.7150606414445912 and parameters: {'x': 1.7150606414445912}. Best is trial 0 with value: 1.7150606414445912.\u001b[0m\n",
      "\u001b[32m[I 2026-01-20 15:39:00,520]\u001b[0m Trial 1 finished with value: 0.43349053614892497 and parameters: {'x': 0.43349053614892497}. Best is trial 1 with value: 0.43349053614892497.\u001b[0m\n",
      "\u001b[32m[I 2026-01-20 15:39:00,522]\u001b[0m Trial 2 finished with value: 0.5046264956700108 and parameters: {'x': 0.5046264956700108}. Best is trial 1 with value: 0.43349053614892497.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: trial.Trial):\n",
    "    x = trial.suggest_float(\"x\", 0, 10)\n",
    "    return x\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters saved to hyperparams.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "hyperparams = {\n",
    "    'FetchReachDense-v4': {\n",
    "        'n_timesteps': 1000,\n",
    "        'policy': 'MultiInputPolicy',\n",
    "        'noise_type': 'ornstein-uhlenbeck',\n",
    "        'noise_std': 0.5,\n",
    "        'gradient_steps': 1,\n",
    "        'train_freq': 1,\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 256,\n",
    "        'policy_kwargs': \"dict(net_arch=[32, 32])\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('hyperparams.yaml', 'w') as f:\n",
    "    yaml.dump(hyperparams, f, sort_keys=False)\n",
    "\n",
    "print(\"Hyperparameters saved to hyperparams.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-20 16:02:38,117]\u001b[0m A new study created in memory with name: no-name-e0e3f097-8b8f-47ee-91d5-c2dca2cfbb79\u001b[0m\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "Best trial: 0. Best value: -1.74981:   3%|▎         | 1/30 [00:51<24:48, 51.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-20 16:03:29,475]\u001b[0m Trial 0 finished with value: -1.749806179665029 and parameters: {'learning_rate': 0.004668562400418924, 'batch_size': 256, 'tau': 0.0015164643247002746, 'gamma': 0.9862852046927062, 'noise_std': 0.22829117172612534, 'net_arch': 'small'}. Best is trial 0 with value: -1.749806179665029.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -1.74981:   7%|▋         | 2/30 [01:33<21:21, 45.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-20 16:04:11,341]\u001b[0m Trial 1 finished with value: -6.676471281796694 and parameters: {'learning_rate': 2.582549765640076e-05, 'batch_size': 64, 'tau': 0.0013009718473785015, 'gamma': 0.9773592450065514, 'noise_std': 0.07071599744468342, 'net_arch': 'small'}. Best is trial 0 with value: -1.749806179665029.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -1.74981:  10%|█         | 3/30 [02:17<20:16, 45.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-20 16:04:55,567]\u001b[0m Trial 2 finished with value: -2.961957621015608 and parameters: {'learning_rate': 0.0004556591488958037, 'batch_size': 128, 'tau': 0.0026721702977186095, 'gamma': 0.9596857722937998, 'noise_std': 0.1298309701377535, 'net_arch': 'small'}. Best is trial 0 with value: -1.749806179665029.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -1.74981:  13%|█▎        | 4/30 [03:05<20:03, 46.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-20 16:05:43,740]\u001b[0m Trial 3 finished with value: -8.914393394440413 and parameters: {'learning_rate': 3.0074405002963306e-05, 'batch_size': 128, 'tau': 0.007990577762987245, 'gamma': 0.9739521664159131, 'noise_std': 0.4368345022218849, 'net_arch': 'small'}. Best is trial 0 with value: -1.749806179665029.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -1.74981:  17%|█▋        | 5/30 [04:25<24:21, 58.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-20 16:07:03,733]\u001b[0m Trial 4 finished with value: -4.552997749112547 and parameters: {'learning_rate': 4.190342217258564e-05, 'batch_size': 128, 'tau': 0.017219851816823308, 'gamma': 0.9565638417359934, 'noise_std': 0.10320732044803821, 'net_arch': 'large'}. Best is trial 0 with value: -1.749806179665029.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -1.74981:  20%|██        | 6/30 [05:11<21:38, 54.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-20 16:07:49,358]\u001b[0m Trial 5 finished with value: -4.091273233853281 and parameters: {'learning_rate': 0.00012011667665234712, 'batch_size': 128, 'tau': 0.0018127545012462058, 'gamma': 0.9628910029911988, 'noise_std': 0.10373343357040972, 'net_arch': 'small'}. Best is trial 0 with value: -1.749806179665029.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -1.74981:  20%|██        | 6/30 [05:56<23:46, 59.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2026-01-20 16:08:34,711]\u001b[0m Trial 6 failed with parameters: {'learning_rate': 0.0015114807441251564, 'batch_size': 256, 'tau': 0.0015576873553844327, 'gamma': 0.9888236347993589, 'noise_std': 0.23073320173292075, 'net_arch': 'small'} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\"\u001b[0m, line \u001b[35m206\u001b[0m, in \u001b[35m_run_trial\u001b[0m\n",
      "    value_or_values = func(trial)\n",
      "  File \u001b[35m\"/var/folders/s1/fhmv_2b17yd1pk7g4y8sw2240000gn/T/ipykernel_31320/1191017117.py\"\u001b[0m, line \u001b[35m43\u001b[0m, in \u001b[35mobjective\u001b[0m\n",
      "    \u001b[31mmodel.learn\u001b[0m\u001b[1;31m(total_timesteps=25000)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/ddpg/ddpg.py\"\u001b[0m, line \u001b[35m126\u001b[0m, in \u001b[35mlearn\u001b[0m\n",
      "    return \u001b[31msuper().learn\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mtotal_timesteps=total_timesteps,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<4 lines>...\n",
      "        \u001b[1;31mprogress_bar=progress_bar,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/td3/td3.py\"\u001b[0m, line \u001b[35m227\u001b[0m, in \u001b[35mlearn\u001b[0m\n",
      "    return \u001b[31msuper().learn\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mtotal_timesteps=total_timesteps,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<4 lines>...\n",
      "        \u001b[1;31mprogress_bar=progress_bar,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/off_policy_algorithm.py\"\u001b[0m, line \u001b[35m335\u001b[0m, in \u001b[35mlearn\u001b[0m\n",
      "    rollout = self.collect_rollouts(\n",
      "        self.env,\n",
      "    ...<5 lines>...\n",
      "        log_interval=log_interval,\n",
      "    )\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/off_policy_algorithm.py\"\u001b[0m, line \u001b[35m568\u001b[0m, in \u001b[35mcollect_rollouts\u001b[0m\n",
      "    new_obs, rewards, dones, infos = \u001b[31menv.step\u001b[0m\u001b[1;31m(actions)\u001b[0m\n",
      "                                     \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mstep\u001b[0m\n",
      "    return \u001b[31mself.step_wait\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\u001b[0m, line \u001b[35m59\u001b[0m, in \u001b[35mstep_wait\u001b[0m\n",
      "    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = \u001b[31mself.envs[env_idx].step\u001b[0m\u001b[1;31m(  # type: ignore[assignment]\u001b[0m\n",
      "                                                                                  \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        \u001b[1;31mself.actions[env_idx]\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/monitor.py\"\u001b[0m, line \u001b[35m94\u001b[0m, in \u001b[35mstep\u001b[0m\n",
      "    observation, reward, terminated, truncated, info = \u001b[31mself.env.step\u001b[0m\u001b[1;31m(action)\u001b[0m\n",
      "                                                       \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\"\u001b[0m, line \u001b[35m125\u001b[0m, in \u001b[35mstep\u001b[0m\n",
      "    observation, reward, terminated, truncated, info = \u001b[31mself.env.step\u001b[0m\u001b[1;31m(action)\u001b[0m\n",
      "                                                       \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\"\u001b[0m, line \u001b[35m393\u001b[0m, in \u001b[35mstep\u001b[0m\n",
      "    return \u001b[31msuper().step\u001b[0m\u001b[1;31m(action)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium/core.py\"\u001b[0m, line \u001b[35m327\u001b[0m, in \u001b[35mstep\u001b[0m\n",
      "    return \u001b[31mself.env.step\u001b[0m\u001b[1;31m(action)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\"\u001b[0m, line \u001b[35m285\u001b[0m, in \u001b[35mstep\u001b[0m\n",
      "    return \u001b[31mself.env.step\u001b[0m\u001b[1;31m(action)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium_robotics/envs/robot_env.py\"\u001b[0m, line \u001b[35m135\u001b[0m, in \u001b[35mstep\u001b[0m\n",
      "    \u001b[31mself._mujoco_step\u001b[0m\u001b[1;31m(action)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/jaydenkm/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium_robotics/envs/robot_env.py\"\u001b[0m, line \u001b[35m341\u001b[0m, in \u001b[35m_mujoco_step\u001b[0m\n",
      "    \u001b[31mself._mujoco.mj_step\u001b[0m\u001b[1;31m(self.model, self.data, nstep=self.n_substeps)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n",
      "\u001b[33m[W 2026-01-20 16:08:34,717]\u001b[0m Trial 6 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_reward\n\u001b[32m     49\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_trial.value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_trial.params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:68\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:165\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:263\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    256\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    259\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    262\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:206\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    208\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    209\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     24\u001b[39m noise = OrnsteinUhlenbeckActionNoise(\n\u001b[32m     25\u001b[39m     mean=np.zeros(env.action_space.shape[\u001b[32m0\u001b[39m]),\n\u001b[32m     26\u001b[39m     sigma=params[\u001b[33m\"\u001b[39m\u001b[33mnoise_std\u001b[39m\u001b[33m\"\u001b[39m] * np.ones(env.action_space.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m model = DDPG(\n\u001b[32m     30\u001b[39m     policy=\u001b[33m\"\u001b[39m\u001b[33mMultiInputPolicy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m     env=env,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     verbose=\u001b[32m0\u001b[39m,\n\u001b[32m     41\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=\u001b[32m10\u001b[39m)\n\u001b[32m     45\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/ddpg/ddpg.py:126\u001b[39m, in \u001b[36mDDPG.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[32m    119\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    125\u001b[39m ) -> SelfDDPG:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/td3/td3.py:227\u001b[39m, in \u001b[36mTD3.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[32m    220\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    226\u001b[39m ) -> SelfTD3:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/off_policy_algorithm.py:335\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     rollout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout.continue_training:\n\u001b[32m    346\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/off_policy_algorithm.py:568\u001b[39m, in \u001b[36mOffPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[39m\n\u001b[32m    565\u001b[39m actions, buffer_actions = \u001b[38;5;28mself\u001b[39m._sample_action(learning_starts, action_noise, env.num_envs)\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    571\u001b[39m num_collected_steps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/stable_baselines3/common/monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium_robotics/envs/robot_env.py:135\u001b[39m, in \u001b[36mBaseRobotEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    132\u001b[39m action = np.clip(action, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m._set_action(action)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mujoco_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m._step_callback()\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/school/proj_situated/robotics_rl/.venv/lib/python3.13/site-packages/gymnasium_robotics/envs/robot_env.py:341\u001b[39m, in \u001b[36mMujocoRobotEnv._mujoco_step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_mujoco_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mujoco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmj_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_substeps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from stable_baselines3 import DDPG, A2C, PPO\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256]),\n",
    "        \"tau\": trial.suggest_float(\"tau\", 0.001, 0.05, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.95, 0.995),\n",
    "        \"noise_std\": trial.suggest_float(\"noise_std\", 0.05, 0.5),\n",
    "        \"net_arch\": trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"large\"]),\n",
    "    }\n",
    "    \n",
    "    net_arch_map = {\"small\": [64, 64], \"medium\": [256, 256], \"large\": [400, 300]}\n",
    "    \n",
    "    env = gym.make(\"FetchReachDense-v4\")\n",
    "    noise = OrnsteinUhlenbeckActionNoise(\n",
    "        mean=np.zeros(env.action_space.shape[0]),\n",
    "        sigma=params[\"noise_std\"] * np.ones(env.action_space.shape[0])\n",
    "    )\n",
    "    \n",
    "    model = DDPG(\n",
    "        policy=\"MultiInputPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        tau=params[\"tau\"],\n",
    "        gamma=params[\"gamma\"],\n",
    "        buffer_size=100000,\n",
    "        learning_starts=1000,\n",
    "        action_noise=noise,\n",
    "        policy_kwargs=dict(net_arch=net_arch_map[params[\"net_arch\"]]),\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=25000)\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    env.close()\n",
    "    \n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"Best reward: {study.best_trial.value}\")\n",
    "print(f\"Best params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train the Agent\n",
    "\n",
    "Train a DDPG agent on the FetchReachDense-v4 environment. Training logs are saved to `logs/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code in the terminal in ~/project_path/src/robotics_rl for training the agent\n",
    "\n",
    "```\n",
    "python -m rl_zoo3.train --algo ddpg --env FetchReachDense-v4 \\\n",
    "    --gym-packages gymnasium_robotics -c hyperparams.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluate the Agent\n",
    "\n",
    "Run the trained agent and see its performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following for evaluating the agent\n",
    "\n",
    "```\n",
    "!python -m rl_zoo3.train --algo ddpg --env FetchReachDense-v4 \\\n",
    "    --gym-packages gymnasium_robotics -c hyperparams.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Record and View Video\n",
    "\n",
    "Record a video of the trained policy to visually evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_video.py is a wrapper that pre-loads gymnasium_robotics\n",
    "# (rl_zoo3.record_video doesn't support --gym-packages)\n",
    "!python record_video.py --algo ddpg --env FetchReachDense-v4 -f logs/ -n 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_videos(video_path, prefix=\"\"):\n",
    "    \"\"\"Display MP4 videos from a folder in the notebook.\"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(f\"{prefix}*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes()).decode('ascii')\n",
    "        html.append(f'''<video alt=\"{mp4}\" autoplay loop controls style=\"height: 400px;\">\n",
    "            <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\" />\n",
    "        </video>''')\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the recorded video\n",
    "# Update the path if your experiment ID differs (check logs/ddpg/ folder)\n",
    "show_videos('logs/ddpg/FetchReachDense-v4_1/videos/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Commands\n",
    "\n",
    "```bash\n",
    "# Train with a different algorithm\n",
    "python -m rl_zoo3.train --algo sac --env FetchReachDense-v4 --gym-packages gymnasium_robotics -c hyperparams.yaml\n",
    "\n",
    "# Train with a specific seed (for reproducibility)\n",
    "python -m rl_zoo3.train --algo ddpg --env FetchReachDense-v4 --gym-packages gymnasium_robotics -c hyperparams.yaml --seed 42\n",
    "\n",
    "# Load best model instead of final model\n",
    "python -m rl_zoo3.enjoy --algo ddpg --env FetchReachDense-v4 --gym-packages gymnasium_robotics -f logs/ --load-best\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics-rl-py3.13 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
